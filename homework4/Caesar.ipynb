{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99c8ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import shuffle\n",
    "import time\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "482c9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "STRING_SIZE = 60\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.1\n",
    "FILE_NAME = \"./3pigs.txt\"\n",
    "CAESAR_N = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c7acb115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet is:\n",
      " Т Р И   П О С Е Н К А \n",
      " Ж и л - б ы н а с в е т р п о к . В д г , у ь з м х Д ж З я : ф й ш ц —   Я М Г ч ю У ! щ \t Х Л э Ч ? Э ъ Б « Ф » 1 2 3 4 5 6 7 8 9 0 P R E F A C S U O I N G t h a T r u i s w o m n e g d f p c l y v b k ; B z W H ( j ) \" V L ' D Y K q M x J _ Q X [ ] Z ä = æ ë é Æ – * ё … Ю ’ „ “ Ш ” è ê ï ç à Ц Ь â á\n",
      " 164 chars\n"
     ]
    }
   ],
   "source": [
    "class Alphabet(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.letters = \"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.letters)\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.letters\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, int):\n",
    "            return self.letters[item % len(self.letters)]\n",
    "        elif isinstance(item, str):\n",
    "            return self.letters.find(item)\n",
    "\n",
    "    def __str__(self):\n",
    "        letters = \" \".join(self.letters)\n",
    "        return f\"Alphabet is:\\n {letters}\\n {len(self)} chars\"\n",
    "\n",
    "    def load_from_file(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            while True:\n",
    "                text = file.read(STRING_SIZE)\n",
    "                if not text:\n",
    "                    break\n",
    "                for ch in text:\n",
    "                    if ch not in self.letters:\n",
    "                        self.letters += ch\n",
    "        return self\n",
    "\n",
    "\n",
    "ALPHABET = Alphabet().load_from_file(FILE_NAME)\n",
    "print(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "251fc935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, raw_data, alphabet):\n",
    "        super().__init__()\n",
    "        self._len = len(raw_data)\n",
    "        self.y = torch.tensor(\n",
    "            [[alphabet[ch] for ch in line] for line in raw_data]\n",
    "        ).to(DEVICE)\n",
    "        self.x = torch.tensor(\n",
    "            [[i + CAESAR_N for i in line] for line in self.y]\n",
    "        ).to(DEVICE)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5819cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_array(file_path, step):\n",
    "    text_array = []\n",
    "    with open(file_path) as file:\n",
    "        while True:\n",
    "            text = file.read(STRING_SIZE)\n",
    "            if not text:\n",
    "                break\n",
    "            text_array.append(text)\n",
    "    del text_array[-1]\n",
    "    return text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7b16c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = get_text_array(FILE_NAME, STRING_SIZE)\n",
    "shuffle(raw_data)\n",
    "_10_percent = math.ceil(len(raw_data) * 0.1)\n",
    "val_data = raw_data[:_10_percent]\n",
    "raw_data = raw_data[_10_percent:]\n",
    "_20_percent = math.ceil(len(raw_data) * 0.2)\n",
    "test_data = raw_data[:_20_percent]\n",
    "train_data = raw_data[_20_percent:]\n",
    "\n",
    "Y_val = torch.tensor([[ALPHABET[ch] for ch in line] for line in val_data])\n",
    "X_val = torch.tensor([[i + CAESAR_N for i in line] for line in Y_val])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    SentenceDataset(\n",
    "        train_data, ALPHABET\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    SentenceDataset(\n",
    "        test_data, ALPHABET\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88888cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(len(ALPHABET) + CAESAR_N, 32)\n",
    "        self.rnn = torch.nn.RNN(32, 128, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(128, len(ALPHABET) + CAESAR_N)\n",
    "\n",
    "    def forward(self, sentence, state=None):\n",
    "        embed = self.embed(sentence)\n",
    "        o, h = self.rnn(embed)\n",
    "        return self.linear(o)\n",
    "    \n",
    "    \n",
    "\n",
    "class RnnFlex(torch.nn.Module):\n",
    "                       \n",
    "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = torch.nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
    "        self.output = torch.nn.Linear(num_hiddens, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        _, state = self.hidden(out)  \n",
    "        predictions = self.output(state[0])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9e45f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel().to(DEVICE)\n",
    "loss = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75683b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 675.8390, acc: 0.9537 | test loss: 35.4452, test acc: 0.9910 | 37.45 sec.\n",
      "Epoch: 1, train loss: 91.3472, acc: 0.9949 | test loss: 15.5549, test acc: 0.9974 | 37.99 sec.\n",
      "Epoch: 2, train loss: 45.1953, acc: 0.9985 | test loss: 8.7967, test acc: 0.9989 | 37.17 sec.\n",
      "Epoch: 3, train loss: 27.4798, acc: 0.9991 | test loss: 5.8567, test acc: 0.9992 | 37.49 sec.\n",
      "Epoch: 4, train loss: 19.2780, acc: 0.9993 | test loss: 4.3502, test acc: 0.9993 | 37.77 sec.\n",
      "Epoch: 5, train loss: 14.7749, acc: 0.9994 | test loss: 3.4447, test acc: 0.9995 | 37.72 sec.\n",
      "Epoch: 6, train loss: 11.9113, acc: 0.9996 | test loss: 2.8307, test acc: 0.9996 | 37.81 sec.\n",
      "Epoch: 7, train loss: 9.9264, acc: 0.9997 | test loss: 2.3904, test acc: 0.9997 | 39.27 sec.\n",
      "Epoch: 8, train loss: 8.4788, acc: 0.9997 | test loss: 2.0596, test acc: 0.9997 | 37.71 sec.\n",
      "Epoch: 9, train loss: 7.3795, acc: 0.9998 | test loss: 1.8044, test acc: 0.9998 | 39.10 sec.\n",
      "Epoch: 10, train loss: 6.5295, acc: 0.9998 | test loss: 1.6019, test acc: 0.9998 | 37.35 sec.\n",
      "Epoch: 11, train loss: 5.8502, acc: 0.9998 | test loss: 1.4381, test acc: 0.9998 | 37.21 sec.\n",
      "Epoch: 12, train loss: 5.2980, acc: 0.9998 | test loss: 1.3046, test acc: 0.9998 | 37.30 sec.\n",
      "Epoch: 13, train loss: 4.8445, acc: 0.9998 | test loss: 1.1936, test acc: 0.9998 | 38.07 sec.\n",
      "Epoch: 14, train loss: 4.4624, acc: 0.9999 | test loss: 1.0999, test acc: 0.9999 | 37.61 sec.\n",
      "Epoch: 15, train loss: 4.1130, acc: 0.9999 | test loss: 1.0200, test acc: 0.9999 | 38.11 sec.\n",
      "Epoch: 16, train loss: 3.8545, acc: 0.9999 | test loss: 0.9504, test acc: 0.9999 | 37.53 sec.\n",
      "Epoch: 17, train loss: 3.6066, acc: 0.9999 | test loss: 0.8894, test acc: 0.9999 | 37.38 sec.\n",
      "Epoch: 18, train loss: 3.3855, acc: 0.9999 | test loss: 0.8353, test acc: 0.9999 | 37.16 sec.\n",
      "Epoch: 19, train loss: 3.1900, acc: 0.9999 | test loss: 0.7869, test acc: 0.9999 | 37.24 sec.\n",
      "Epoch: 20, train loss: 3.0130, acc: 0.9999 | test loss: 0.7434, test acc: 0.9999 | 37.27 sec.\n",
      "Epoch: 21, train loss: 2.8530, acc: 0.9999 | test loss: 0.7041, test acc: 0.9999 | 37.43 sec.\n",
      "Epoch: 22, train loss: 2.7072, acc: 0.9999 | test loss: 0.6683, test acc: 0.9999 | 37.42 sec.\n",
      "Epoch: 23, train loss: 2.5734, acc: 0.9999 | test loss: 0.6359, test acc: 0.9999 | 37.34 sec.\n",
      "Epoch: 24, train loss: 2.4517, acc: 0.9999 | test loss: 0.6062, test acc: 0.9999 | 37.26 sec.\n",
      "Epoch: 25, train loss: 2.3401, acc: 0.9999 | test loss: 0.5790, test acc: 0.9999 | 37.27 sec.\n",
      "Epoch: 26, train loss: 2.2369, acc: 0.9999 | test loss: 0.5540, test acc: 0.9999 | 37.35 sec.\n",
      "Epoch: 27, train loss: 2.1416, acc: 0.9999 | test loss: 0.5310, test acc: 1.0000 | 37.11 sec.\n",
      "Epoch: 28, train loss: 2.0525, acc: 1.0000 | test loss: 0.5098, test acc: 1.0000 | 37.18 sec.\n",
      "Epoch: 29, train loss: 1.9721, acc: 1.0000 | test loss: 0.4901, test acc: 1.0000 | 37.96 sec.\n",
      "Epoch: 30, train loss: 1.8960, acc: 1.0000 | test loss: 0.4719, test acc: 1.0000 | 37.36 sec.\n",
      "Epoch: 31, train loss: 1.8253, acc: 1.0000 | test loss: 0.4548, test acc: 1.0000 | 37.38 sec.\n",
      "Epoch: 32, train loss: 1.7591, acc: 1.0000 | test loss: 0.4391, test acc: 1.0000 | 37.08 sec.\n",
      "Epoch: 33, train loss: 1.6980, acc: 1.0000 | test loss: 0.4243, test acc: 1.0000 | 37.35 sec.\n",
      "Epoch: 34, train loss: 1.6406, acc: 1.0000 | test loss: 0.4105, test acc: 1.0000 | 37.49 sec.\n",
      "Epoch: 35, train loss: 1.5869, acc: 1.0000 | test loss: 0.3975, test acc: 1.0000 | 38.55 sec.\n",
      "Epoch: 36, train loss: 1.5366, acc: 1.0000 | test loss: 0.3854, test acc: 1.0000 | 37.30 sec.\n",
      "Epoch: 37, train loss: 1.4893, acc: 1.0000 | test loss: 0.3737, test acc: 1.0000 | 37.57 sec.\n",
      "Epoch: 38, train loss: 1.4450, acc: 1.0000 | test loss: 0.3633, test acc: 1.0000 | 37.26 sec.\n",
      "Epoch: 39, train loss: 1.4034, acc: 1.0000 | test loss: 0.3532, test acc: 1.0000 | 37.28 sec.\n",
      "Epoch: 40, train loss: 1.3640, acc: 1.0000 | test loss: 0.3437, test acc: 1.0000 | 37.62 sec.\n",
      "Epoch: 41, train loss: 1.3272, acc: 1.0000 | test loss: 0.3346, test acc: 1.0000 | 37.42 sec.\n",
      "Epoch: 42, train loss: 1.2922, acc: 1.0000 | test loss: 0.3259, test acc: 1.0000 | 37.57 sec.\n",
      "Epoch: 43, train loss: 1.2592, acc: 1.0000 | test loss: 0.3178, test acc: 1.0000 | 37.11 sec.\n",
      "Epoch: 44, train loss: 1.2279, acc: 1.0000 | test loss: 0.3104, test acc: 1.0000 | 37.77 sec.\n",
      "Epoch: 45, train loss: 1.1981, acc: 1.0000 | test loss: 0.3032, test acc: 1.0000 | 37.75 sec.\n",
      "Epoch: 46, train loss: 1.1700, acc: 1.0000 | test loss: 0.2962, test acc: 1.0000 | 37.52 sec.\n",
      "Epoch: 47, train loss: 1.1432, acc: 1.0000 | test loss: 0.2896, test acc: 1.0000 | 37.36 sec.\n",
      "Epoch: 48, train loss: 1.1176, acc: 1.0000 | test loss: 0.2833, test acc: 1.0000 | 37.78 sec.\n",
      "Epoch: 49, train loss: 1.0935, acc: 1.0000 | test loss: 0.2773, test acc: 1.0000 | 37.50 sec.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc, iter_num = .0, .0, .0\n",
    "    start_epoch_time = time.time()\n",
    "    model.train()\n",
    "    for x_in, y_in in train_dl:\n",
    "        x_in = x_in\n",
    "        y_in = y_in.view(1, -1).squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(x_in).view(-1, len(ALPHABET) + CAESAR_N)\n",
    "        l = loss(out, y_in)\n",
    "        train_loss += l.item()\n",
    "        batch_acc = (out.argmax(dim=1) == y_in)\n",
    "        train_acc += batch_acc.sum().item() / batch_acc.shape[0]\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        iter_num += 1\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, train loss: {train_loss:.4f}, acc: \"\n",
    "        f\"{train_acc / iter_num:.4f}\",\n",
    "        end=\" | \"\n",
    "    )\n",
    "    test_loss, test_acc, iter_num = .0, .0, .0\n",
    "    model.eval()\n",
    "    for x_in, y_in in test_dl:\n",
    "        x_in = x_in\n",
    "        y_in = y_in.view(1, -1).squeeze()\n",
    "        out = model.forward(x_in).view(-1, len(ALPHABET) + CAESAR_N)\n",
    "        l = loss(out, y_in)\n",
    "        test_loss += l.item()\n",
    "        batch_acc = (out.argmax(dim=1) == y_in)\n",
    "        test_acc += batch_acc.sum().item() / batch_acc.shape[0]\n",
    "        iter_num += 1\n",
    "    print(\n",
    "        f\"test loss: {test_loss:.4f}, test acc: {test_acc / iter_num:.4f} | \"\n",
    "        f\"{time.time() - start_epoch_time:.2f} sec.\"\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79daa0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "97881f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст:           Судьба.\n",
      "Был только один выход, ибо наши жизни сплелись в слишком запутанный узел гнева и блаженства, \n",
      "чтобы решить все как-нибудь иначе. Доверимся жребию: орел — и мы поженимся, решка — и мы расстанемся навсегда.\n",
      "Монетка была подброшена. Она звякнула, завертелась и остановилась. Орел.\n",
      "Мы уставились на нее с недоумением.\n",
      "Затем, в один голос, мы сказали: «Может, еще разок?». Джей Рип\n",
      "Elizabeth II was Queen of the United Kingdom and other Commonwealth realms from 6 February 1952 until her death in 2022. \n",
      "She was queen regnant of 32 sovereign states during her lifetime and 15 at the time of her death.\n",
      "Her reign of 70 years and 214 days is the longest of any British monarch, \n",
      "the longest recorded of any female head of state in history, \n",
      "and the second-longest verified reign of any monarch in history.\n",
      "\n",
      "\n",
      "Зашифрованный текст:      Нз,мнвдиФабОп.бмВ.О.,-сОтаж.,ьО-н.Осв—-Оя-хс-Оекбрб-емОтОеб-—В.ДОхвкзпвссацОзхрбОусртвО-ОнбвярсептвьОиУп.наОор—-пмОтерОВвВыс-нз,мО-свУрдОЗ.тро-ДефОяорн-!йО.орбОЯО-ОДаОк.ярс-ДефьОор—ВвОЯО-ОДаОовеепвсрДефОсвтеру,вдич.српВвОнабвОк.,но.—рсвдОЕсвОхтфВсзбвьОхвтропрбвемО-О.епвс.т-бвемдОЕорбдичаОзепвт-б-емОсвОсррОеОср,.зДрс-рДди:впрДьОтО.,-сОу.б.еьОДаОеВвхвб-йО»ч.ярпьОрХрОовх.Въ2дОЗярцО -киAvwHr;daTОGGОmroО[sddgОncОaTdОIgwadpОMwgfpneОrgpОnaTdiОUneengmdrvaTОidrveoОcineО8ОCd;isribО3P74ОsgawvОTdiОpdraTОwgО4R44дОиOTdОmroОxsddgОidfgrgaОncО54ОonkdidwfgОoaradoОpsiwgfОTdiОvwcdawedОrgpО37ОraОaTdОawedОncОTdiОpdraTдиjdiОidwfgОncО9RОbdrioОrgpО436ОprboОwoОaTdОvngfdoaОncОrgbОWiwawoTОengriyTьОиaTdОvngfdoaОidynipdpОncОrgbОcdervdОTdrpОncОoaradОwgОTwoanibьОиrgpОaTdОodyngpыvngfdoaОkdiwcwdpОidwfgОncОrgbОengriyTОwgОTwoanibди\n",
      "\n",
      "Расшифрованный текст:     Судьба.\n",
      "Был только один выход, ибо наши жизни сплелись в слишком запутанный узел гнева и блаженства, \n",
      "чтобы решить все как-нибудь иначе. Доверимся жребию: орел — и мы поженимся, решка — и мы расстанемся навсегда.\n",
      "Монетка была подброшена. Она звякнула, завертелась и остановилась. Орел.\n",
      "Мы уставились на нее с недоумением.\n",
      "Затем, в один голос, мы сказали: «Может, еще разок?». Джей Рип\n",
      "Elizabeth II was Queen of the United Kingdom and other Commonwealth realms from 6 February 1952 until her death in 2022. \n",
      "She was queen regnant of 32 sovereign states during her lifetime and 15 at the time of her death.\n",
      "Her reign of 70 years and 214 days is the longest of any British monarch, \n",
      "the longest recorded of any female head of state in history, \n",
      "and the second-longest verified reign of any monarch in history.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"Судьба.\n",
    "Был только один выход, ибо наши жизни сплелись в слишком запутанный узел гнева и блаженства, \n",
    "чтобы решить все как-нибудь иначе. Доверимся жребию: орел — и мы поженимся, решка — и мы расстанемся навсегда.\n",
    "Монетка была подброшена. Она звякнула, завертелась и остановилась. Орел.\n",
    "Мы уставились на нее с недоумением.\n",
    "Затем, в один голос, мы сказали: «Может, еще разок?». Джей Рип\n",
    "Elizabeth II was Queen of the United Kingdom and other Commonwealth realms from 6 February 1952 until her death in 2022. \n",
    "She was queen regnant of 32 sovereign states during her lifetime and 15 at the time of her death.\n",
    "Her reign of 70 years and 214 days is the longest of any British monarch, \n",
    "the longest recorded of any female head of state in history, \n",
    "and the second-longest verified reign of any monarch in history.\n",
    "\"\"\"\n",
    "sentence_idx = [ALPHABET[i] for i in sentence]\n",
    "encrypted_sentence_idx = [i + CAESAR_N for i in sentence_idx]\n",
    "encrypted_sentence = \"\".join([ALPHABET[i] for i in encrypted_sentence_idx])\n",
    "result = model(torch.tensor([encrypted_sentence_idx]).to(DEVICE)).argmax(dim=2)\n",
    "deencrypted_sentence = \"\".join([ALPHABET[i.item()] for i in result.flatten()])\n",
    "print(f\"\"\"Исходный текст:           {sentence}\n",
    "\"\"\")\n",
    "print(f\"\"\"Зашифрованный текст:      {encrypted_sentence}\n",
    "\"\"\")\n",
    "print(f\"Расшифрованный текст:     {deencrypted_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb3aa447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "wer = load(\"wer\")\n",
    "\n",
    "predictions = [sentence]\n",
    "references = [deencrypted_sentence]\n",
    "wer_score = wer.compute (predictions=predictions, references=references)\n",
    "print(wer_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fce055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
